{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "LOAD THE DATA SET"
      ],
      "metadata": {
        "id": "Y2Ak00X5CO88"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9aBBPFetBiRo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('grammar_correction_pairs.csv')\n",
        "\n",
        "# Prepare training data\n",
        "incorrect_sentences = df['incorrect_sentence'].values\n",
        "correct_sentences = df['correct_sentence'].values"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TOKENIZATION"
      ],
      "metadata": {
        "id": "wZjv3ha-CuFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Tokenize the sentences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(list(incorrect_sentences) + list(correct_sentences))\n",
        "\n",
        "# Convert text to sequences\n",
        "X = tokenizer.texts_to_sequences(incorrect_sentences)\n",
        "y = tokenizer.texts_to_sequences(correct_sentences)\n",
        "\n",
        "# Pad sequences to make them the same length\n",
        "X = pad_sequences(X, padding='post')\n",
        "y = pad_sequences(y, padding='post')"
      ],
      "metadata": {
        "id": "09gcz_JAC1FK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DEFINE THE MODEL"
      ],
      "metadata": {
        "id": "fRS7EdGEC6NT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, RepeatVector, TimeDistributed\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "\n",
        "# Define embedding layer\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=256, input_length=X.shape[1]))\n",
        "\n",
        "# Encoder\n",
        "model.add(LSTM(256))\n",
        "\n",
        "# Decoder\n",
        "model.add(RepeatVector(X.shape[1]))  # Repeat the context vector\n",
        "model.add(LSTM(256, return_sequences=True))\n",
        "model.add(TimeDistributed(Dense(len(tokenizer.word_index) + 1, activation='softmax')))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFtRX4tRDDp5",
        "outputId": "8e76cdec-cc00-4782-d85a-35c6961dafb3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRAIN THE MODEL"
      ],
      "metadata": {
        "id": "ZrpYtUWUDFqz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model.fit(X, y, batch_size=64, epochs=20, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rcoKs5PDHbo",
        "outputId": "cd8f2747-a105-4edb-daad-cb5e74f08e9b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 124ms/step - accuracy: 0.3672 - loss: 3.0709 - val_accuracy: 0.4629 - val_loss: 4.0673\n",
            "Epoch 2/20\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 115ms/step - accuracy: 0.7980 - loss: 0.8388 - val_accuracy: 0.4239 - val_loss: 4.6955\n",
            "Epoch 3/20\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 113ms/step - accuracy: 0.8238 - loss: 0.6603 - val_accuracy: 0.4768 - val_loss: 4.6292\n",
            "Epoch 4/20\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 121ms/step - accuracy: 0.8832 - loss: 0.4588 - val_accuracy: 0.5253 - val_loss: 4.3920\n",
            "Epoch 5/20\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 121ms/step - accuracy: 0.9140 - loss: 0.3253 - val_accuracy: 0.5280 - val_loss: 4.4326\n",
            "Epoch 6/20\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 125ms/step - accuracy: 0.9237 - loss: 0.2595 - val_accuracy: 0.5537 - val_loss: 4.2018\n",
            "Epoch 7/20\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 123ms/step - accuracy: 0.9300 - loss: 0.2247 - val_accuracy: 0.5575 - val_loss: 4.4543\n",
            "Epoch 8/20\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 115ms/step - accuracy: 0.9475 - loss: 0.1715 - val_accuracy: 0.5728 - val_loss: 4.3238\n",
            "Epoch 9/20\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 122ms/step - accuracy: 0.9634 - loss: 0.1345 - val_accuracy: 0.5621 - val_loss: 4.4554\n",
            "Epoch 10/20\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 114ms/step - accuracy: 0.9771 - loss: 0.0945 - val_accuracy: 0.5735 - val_loss: 4.4782\n",
            "Epoch 11/20\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 118ms/step - accuracy: 0.9864 - loss: 0.0598 - val_accuracy: 0.5802 - val_loss: 4.4557\n",
            "Epoch 12/20\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 121ms/step - accuracy: 0.9873 - loss: 0.0516 - val_accuracy: 0.5828 - val_loss: 4.3937\n",
            "Epoch 13/20\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 113ms/step - accuracy: 0.9905 - loss: 0.0396 - val_accuracy: 0.5917 - val_loss: 4.4264\n",
            "Epoch 14/20\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 122ms/step - accuracy: 0.9965 - loss: 0.0216 - val_accuracy: 0.5802 - val_loss: 4.5492\n",
            "Epoch 15/20\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 120ms/step - accuracy: 0.9954 - loss: 0.0227 - val_accuracy: 0.5494 - val_loss: 5.0222\n",
            "Epoch 16/20\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 112ms/step - accuracy: 0.9959 - loss: 0.0228 - val_accuracy: 0.5998 - val_loss: 4.4320\n",
            "Epoch 17/20\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 121ms/step - accuracy: 0.9988 - loss: 0.0089 - val_accuracy: 0.6024 - val_loss: 4.4261\n",
            "Epoch 18/20\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 114ms/step - accuracy: 0.9991 - loss: 0.0071 - val_accuracy: 0.5928 - val_loss: 4.6298\n",
            "Epoch 19/20\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 121ms/step - accuracy: 0.9997 - loss: 0.0050 - val_accuracy: 0.5944 - val_loss: 4.6194\n",
            "Epoch 20/20\n",
            "\u001b[1m180/180\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 122ms/step - accuracy: 0.9997 - loss: 0.0035 - val_accuracy: 0.5927 - val_loss: 4.6177\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x788d709bae00>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "INFERENCE"
      ],
      "metadata": {
        "id": "6bmyeftyKytG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to predict the corrected sentence\n",
        "def correct_grammar(input_sentence):\n",
        "    seq_input = tokenizer.texts_to_sequences([input_sentence])\n",
        "    padded_input = pad_sequences(seq_input, padding='post', maxlen=X.shape[1])\n",
        "\n",
        "    pred = model.predict(padded_input)\n",
        "\n",
        "    # Convert prediction to words\n",
        "    pred_sentence = ' '.join([tokenizer.index_word.get(idx, '') for idx in pred[0].argmax(axis=-1)])\n",
        "    return pred_sentence"
      ],
      "metadata": {
        "id": "xY2HGL8zK0ld"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "USER INPUT"
      ],
      "metadata": {
        "id": "4qUMZlAnK-Hd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get user input and correct grammar\n",
        "user_input = input(\"Enter a sentence with possible grammar errors: \")\n",
        "corrected_sentence = correct_grammar(user_input)\n",
        "print(\"Corrected sentence:\", corrected_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4CYtGIHLBqb",
        "outputId": "58ea4d08-5888-43ee-a1f3-635c35a38296"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence with possible grammar errors: මම යන්නෙමු\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 390ms/step\n",
            "Corrected sentence: මම යන්නෙමි     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grammar Correction Function for Paragraphs:"
      ],
      "metadata": {
        "id": "s7QBAN6lMq7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Function to split paragraph into sentences by period (.)\n",
        "def split_paragraph_into_sentences(paragraph):\n",
        "    # Split the paragraph by punctuation mark (.)\n",
        "    sentences = paragraph.split(\".\")\n",
        "\n",
        "    # Clean up sentences (remove empty sentences after split)\n",
        "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
        "    return sentences\n",
        "\n",
        "# Function to correct a single sentence (using the trained model)\n",
        "def correct_grammar_for_sentence(input_sentence, tokenizer, model, max_input_length):\n",
        "    # Tokenize and pad the input sentence\n",
        "    seq_input = tokenizer.texts_to_sequences([input_sentence])\n",
        "    padded_input = pad_sequences(seq_input, padding='post', maxlen=max_input_length)\n",
        "\n",
        "    # Predict the corrected sentence\n",
        "    pred = model.predict(padded_input)\n",
        "\n",
        "    # Convert prediction to words\n",
        "    pred_sentence = ' '.join([tokenizer.index_word.get(idx, '') for idx in pred[0].argmax(axis=-1)])\n",
        "    return pred_sentence\n",
        "\n",
        "# Function to correct grammar in an entire paragraph\n",
        "def correct_grammar_in_paragraph(paragraph, tokenizer, model, max_input_length):\n",
        "    # Step 1: Split the paragraph into sentences\n",
        "    sentences = split_paragraph_into_sentences(paragraph)\n",
        "\n",
        "    # Step 2: Correct each sentence\n",
        "    corrected_sentences = [correct_grammar_for_sentence(sentence, tokenizer, model, max_input_length) for sentence in sentences]\n",
        "\n",
        "    # Step 3: Join the corrected sentences back into a paragraph\n",
        "    corrected_paragraph = '. '.join(corrected_sentences) + '.' if corrected_sentences else ''\n",
        "\n",
        "    return corrected_paragraph"
      ],
      "metadata": {
        "id": "viGx2xphMync"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "# Assuming you have the model and tokenizer already loaded\n",
        "user_paragraph = \"මම යන්නෙමු. අපි යනවා. මම යවන්නෙමි. මම යවන්නෙහි. මම ගියාය. අපි යනවාලා.\"\n",
        "corrected_paragraph = correct_grammar_in_paragraph(user_paragraph, tokenizer, model, X.shape[1])\n",
        "\n",
        "print(\"Original Paragraph: \", user_paragraph)\n",
        "print(\"Corrected Paragraph: \", corrected_paragraph)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNADDUHlM4sL",
        "outputId": "d8946a0d-dfb9-4d93-d210-564254089a73"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "Original Paragraph:  මම යන්නෙමු. අපි යනවා. මම යවන්නෙමි. මම යවන්නෙහි. මම ගියාය. අපි යනවාලා.\n",
            "Corrected Paragraph:  මම යන්නෙමි     . මම යැවෙමි     . මම යමි     . මම යවන්නෙමි     . මම ගියෙමි     . මම යැවෙමි     .\n"
          ]
        }
      ]
    }
  ]
}